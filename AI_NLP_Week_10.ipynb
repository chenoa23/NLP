{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chenoa23/NLP/blob/main/AI_NLP_Week_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##This notebook combines different machine learning and deep learning tasks using both pre-trained models and custom-built neural networks. It first uses the LLaMA 3.2 (3B) model from Hugging Face to generate text based on user prompts, and the LLaMA 3.2 Vision model (11B) to describe an image when given a picture and a question. It also includes two custom models built with TensorFlow. One predicts whether a wine is good based on its features using the Wine Quality dataset, and the other performs sentiment analysis on IMDB movie reviews to tell if a review is positive or negative. The notebook includes data cleaning, training, evaluation, and visualizations for each model. Overall, it shows how different types of data—text, images, and tables—can be handled using AI."
      ],
      "metadata": {
        "id": "ci_tUNfbfFtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "HUGGINGFACE_TOKEN = getpass(\"Enter your Hugging Face token:\")"
      ],
      "metadata": {
        "id": "h5Q_l_Et3ydI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers>=4.45.0\"\n",
        "!pip install pillow  # For handling images with the Vision model"
      ],
      "metadata": {
        "id": "480aNndg3x9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "7uTDNXzv5HQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Tell me about the history of Miami Dade College and who the current President is\"}]\n",
        "outputs = pipe(messages, max_new_tokens=150)\n",
        "response = outputs[0][\"generated_text\"]\n",
        "print(response)"
      ],
      "metadata": {
        "id": "6AvvoExj57ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Extract the assistant's response text\n",
        "response_content = outputs[0][\"generated_text\"][-1][\"content\"] if isinstance(outputs[0][\"generated_text\"], list) else outputs[0][\"generated_text\"]\n",
        "\n",
        "# Define the wrap width\n",
        "wrap_width = 70\n",
        "\n",
        "# Print with formatted output\n",
        "print(\"User:\", messages[0][\"content\"])\n",
        "print(\"\\nAssistant:\\n\")\n",
        "for line in response_content.split(\"\\n\"):\n",
        "    print(textwrap.fill(line, width=wrap_width))"
      ],
      "metadata": {
        "id": "1Jda_6oS57cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"Tell me about the mating habits of the African Honeybee\"}]\n",
        "outputs = pipe(messages, max_new_tokens=150)\n",
        "response = outputs[0][\"generated_text\"]\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tdmcQa9W57W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "asWr3dp99Kul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Extract the assistant's response text\n",
        "response_content = outputs[0][\"generated_text\"][-1][\"content\"] if isinstance(outputs[0][\"generated_text\"], list) else outputs[0][\"generated_text\"]\n",
        "\n",
        "# Define the wrap width\n",
        "wrap_width = 70\n",
        "\n",
        "# Print with formatted output\n",
        "print(\"User:\", messages[0][\"content\"])\n",
        "print(\"\\nAssistant:\\n\")\n",
        "for line in response_content.split(\"\\n\"):\n",
        "    print(textwrap.fill(line, width=wrap_width))"
      ],
      "metadata": {
        "id": "6fd7FRZL9L_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, MllamaForConditionalGeneration\n",
        "\n",
        "vision_model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(vision_model_id)\n",
        "model = MllamaForConditionalGeneration.from_pretrained(vision_model_id, torch_dtype=torch.bfloat16, use_auth_token=HUGGINGFACE_TOKEN)"
      ],
      "metadata": {
        "id": "AVeqsOE0Hn1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example image URL\n",
        "url = \"https://miro.medium.com/v2/resize:fit:2400/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "# Prepare the image input with a text query\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\"},\n",
        "        {\"type\": \"text\", \"text\": \"Can you describe this image?\"}\n",
        "    ]}\n",
        "]\n",
        "\n",
        "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(image, input_text, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(**inputs, max_new_tokens=70)\n",
        "print(processor.decode(output[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "id": "2HhMt0jxHnwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "# Decode and format the output\n",
        "decoded_output = processor.decode(output[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "\n",
        "# Define the wrap width\n",
        "wrap_width = 70\n",
        "\n",
        "# Print formatted output with text wrapping\n",
        "print(\"\\nFormatted Output:\\n\")\n",
        "for line in decoded_output.split(\"\\n\"):\n",
        "    print(textwrap.fill(line, width=wrap_width))"
      ],
      "metadata": {
        "id": "8COQZdYyHnkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGK5dQLOHncd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeKzujWoHnQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHY5f2zVHnHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/fenago/datasets/refs/heads/main/winequalityN.csv')\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "1MfSKagAGpKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "5Zd6wj1fLSL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "J9pAlWspLSEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "nvJdsQrFLR-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "nL1zWhLGOOB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_white_wine'] = [\n",
        "    1 if typ == 'white' else 0 for typ in df['type']]\n",
        "df.drop('type', axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cMtJyvhDLR4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "id": "kuypiq93OddX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['is_good_wine'] = [\n",
        "    1 if quality >= 6 else 0 for quality in df['quality']\n",
        "]\n",
        "df.drop('quality', axis=1, inplace=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NRswOcarOjB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = df.drop('is_good_wine', axis=1)\n",
        "y = df['is_good_wine']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "7TuJE-maQASW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "IKPexBuoQJKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Corrected optimizer parameter\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy,\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.03),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall')\n",
        "    ]\n",
        ")\n",
        "\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100)\n"
      ],
      "metadata": {
        "id": "T9sc7W_gRGRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "rcParams['figure.figsize'] = (18, 8)\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False"
      ],
      "metadata": {
        "id": "QvqJEiCnSlnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(\n",
        "    np.arange(1, 101),\n",
        "    history.history['loss'], label='Loss'\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101),\n",
        "    history.history['accuracy'], label='Accuracy'\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101),\n",
        "    history.history['precision'], label='Precision'\n",
        ")\n",
        "plt.plot(\n",
        "    np.arange(1, 101),\n",
        "    history.history['recall'], label='Recall'\n",
        ")\n",
        "plt.title('Evaluation metrics', size=20)\n",
        "plt.xlabel('Epoch', size=14)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "2SbaD8YLSs05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "fgd3tE7vS9lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_classes = [\n",
        "    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
        "]"
      ],
      "metadata": {
        "id": "Y4i7c96gTEen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, prediction_classes))"
      ],
      "metadata": {
        "id": "3Csm6Ye2UP9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "print(f'Accuracy: {accuracy_score(y_test, prediction_classes):.2f}')\n",
        "print(f'Precision: {precision_score(y_test, prediction_classes):.2f}')\n",
        "print(f'Recall: {recall_score(y_test, prediction_classes):.2f}')"
      ],
      "metadata": {
        "id": "J6jfpAblVz9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "df.sample(5)\n"
      ],
      "metadata": {
        "id": "PZdv44bKW1AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabet characters\n",
        "    return text.lower().strip()\n",
        "\n",
        "# Clean the reviews\n",
        "df['review'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['review'])\n",
        "sequences = tokenizer.texts_to_sequences(df['review'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=200)\n"
      ],
      "metadata": {
        "id": "5nsL8qcTXKFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentiment labels to binary\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "# Splitting the data into features (X) and labels (y)\n",
        "X = padded_sequences\n",
        "y = df['sentiment'].values\n"
      ],
      "metadata": {
        "id": "BQO_hSmVXWNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "4ftez81EXa7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "IdEMrhC3Xb_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(10000, 16, input_length=200),\n",
        "    tf.keras.layers.LSTM(64, return_sequences=True),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "Mwajw9BqXnvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rFsDY1Z3YR7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "VMTCh0c_Y2OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews = [\n",
        "    \"I absolutely loved this movie! The plot was thrilling and the characters were so well developed.\",\n",
        "    \"The film was a disaster. Poor acting and a predictable storyline.\"\n",
        "]\n",
        "\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_reviews)\n",
        "sample_padded = pad_sequences(sample_sequences, maxlen=200)\n",
        "\n",
        "predictions = model.predict(sample_padded)\n",
        "print([\"Positive\" if prob > 0.5 else \"Negative\" for prob in predictions])\n"
      ],
      "metadata": {
        "id": "80NwvqLtZUE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}